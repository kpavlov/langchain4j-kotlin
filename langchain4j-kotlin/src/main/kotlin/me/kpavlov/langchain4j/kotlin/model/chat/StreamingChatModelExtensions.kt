package me.kpavlov.langchain4j.kotlin.model.chat

import dev.langchain4j.model.chat.StreamingChatModel
import dev.langchain4j.model.chat.response.ChatResponse
import dev.langchain4j.model.chat.response.StreamingChatResponseHandler
import dev.langchain4j.service.TokenStream
import kotlinx.coroutines.channels.BufferOverflow
import kotlinx.coroutines.channels.Channel
import kotlinx.coroutines.channels.awaitClose
import kotlinx.coroutines.flow.Flow
import kotlinx.coroutines.flow.buffer
import kotlinx.coroutines.flow.callbackFlow
import kotlinx.coroutines.flow.flow
import me.kpavlov.langchain4j.kotlin.model.chat.request.ChatRequestBuilder
import me.kpavlov.langchain4j.kotlin.model.chat.request.chatRequest
import org.slf4j.LoggerFactory

private val logger = LoggerFactory.getLogger(StreamingChatModel::class.java)

/**
 * Represents different types of replies that can be received from an AI language model during streaming.
 * This sealed interface provides type-safe handling of both intermediate tokens and final completion responses.
 */
public sealed interface StreamingChatModelReply {
    /**
     * Represents a partial response received from an AI language model during a streaming interaction.
     *
     * This data class is used to encapsulate an intermediate token that the model generates as part of its
     * streaming output. Partial responses are often used in scenarios where the model's output is produced
     * incrementally, enabling real-time updates to the user or downstream processes.
     *
     * @property token The string representation of the token generated as part of the streaming process.
     * @see StreamingChatResponseHandler.onPartialResponse
     */
    public data class PartialResponse(
        val token: String,
    ) : StreamingChatModelReply

    /**
     * Represents a final completion response received from the AI language model
     * during the streaming chat process.
     *
     * This data class encapsulates the complete response, which typically contains
     * the final output of a model's reply in the context of a conversation.
     *
     * @property response The final chat response generated by the model.
     * @see StreamingChatResponseHandler.onCompleteResponse
     */
    public data class CompleteResponse(
        val response: ChatResponse,
    ) : StreamingChatModelReply

    /**
     * Represents an error that occurred during the streaming process
     * when generating a reply from the AI language model. This type
     * of reply is used to indicate a failure in the operation and
     * provides details about the cause of the error.
     *
     * @property cause The underlying exception or error that caused the failure.
     * @see StreamingChatResponseHandler.onError
     */
    public data class Error(
        val cause: Throwable,
    ) : StreamingChatModelReply
}

/**
 * Converts a streaming chat language model into a Kotlin [Flow] of [StreamingChatModelReply]
 * events. This extension function provides a coroutine-friendly way to consume streaming responses
 * from the language model.
 *
 * The method uses a provided configuration block to build a chat request
 * and manages the streaming process by handling partial responses, complete
 * responses, and errors through a LC4J's [dev.langchain4j.model.chat.response.StreamingChatResponseHandler].
 *
 * @param bufferCapacity The capacity of the buffer used to store incoming tokens.
 * Default to [Channel.UNLIMITED]
 * @param onBufferOverflow The strategy used to handle buffer overflows when the buffer is full.
 * Default to [BufferOverflow.SUSPEND]. The available strategies are:
 * - [BufferOverflow.SUSPEND]: Suspends the producer until there is space in the buffer. This is
 *   suitable for scenarios where maintaining the order of all emitted items is critical.
 * - [BufferOverflow.DROP_OLDEST]: Drops the oldest item in the buffer to make space for the new
 *   item. This is useful when the latest data is more relevant than older data, such as in real-time
 *   updates or streaming dashboards.
 * - [BufferOverflow.DROP_LATEST]: Drops the new item if the buffer is full. This is appropriate
 *   when older data must be preserved, and losing the latest data is acceptable, such as in logging
 *   or audit trails.
 * @param block A lambda with receiver on [ChatRequestBuilder] used to configure
 * the [dev.langchain4j.model.chat.request.ChatRequest] by adding messages and/or setting parameters.
 *
 * @return A [Flow] of [StreamingChatModelReply], which emits different
 * types of replies during the chat interaction, including partial responses,
 * final responses, and errors.
 */
@JvmOverloads
public fun StreamingChatModel.chatFlow(
    bufferCapacity: Int = Channel.UNLIMITED,
    onBufferOverflow: BufferOverflow = BufferOverflow.SUSPEND,
    block: ChatRequestBuilder.() -> Unit,
): Flow<StreamingChatModelReply> =
    callbackFlow {
        val model = this@chatFlow
        val chatRequest = chatRequest(block)
        val handler =
            object : StreamingChatResponseHandler {
                override fun onPartialResponse(token: String) {
                    logger.trace(
                        me.kpavlov.langchain4j.kotlin.internal.SENSITIVE,
                        "Received partialResponse: {}",
                        token,
                    )
                    trySend(StreamingChatModelReply.PartialResponse(token))
                }

                override fun onCompleteResponse(completeResponse: ChatResponse) {
                    logger.trace(
                        me.kpavlov.langchain4j.kotlin.internal.SENSITIVE,
                        "Received completeResponse: {}",
                        completeResponse,
                    )
                    trySend(StreamingChatModelReply.CompleteResponse(completeResponse))
                    close()
                }

                override fun onError(error: Throwable) {
                    logger.error(
                        "Received error: {}",
                        error.message,
                        error,
                    )
                    trySend(StreamingChatModelReply.Error(error))
                    close(error)
                }
            }

        logger.info("Starting flow...")
        model.chat(chatRequest, handler)

        // This will be called when the flow collection is closed or cancelled.
        awaitClose {
            // cleanup
            logger.info("Flow is canceled")
        }
    }.buffer(bufferCapacity, onBufferOverflow)

public fun TokenStream.asFlow(): Flow<String> =
    flow {
        callbackFlow {
            onPartialResponse { trySend(it) }
            onCompleteResponse { close() }
            onError { close(it) }
            start()
            awaitClose()
        }.buffer(Channel.UNLIMITED).collect(this)
    }

public fun TokenStream.asReplyFlow(): Flow<StreamingChatModelReply> =
    flow {
        callbackFlow<StreamingChatModelReply> {
            onPartialResponse { token ->
                trySend(StreamingChatModelReply.PartialResponse(token))
            }
            onCompleteResponse { response ->
                trySend(StreamingChatModelReply.CompleteResponse(response))
                close()
            }
            onError { throwable -> close(throwable) }
            start()
            awaitClose()
        }.buffer(Channel.UNLIMITED).collect(this)
    }
