package me.kpavlov.langchain4j.kotlin.model.chat

import dev.langchain4j.data.message.AiMessage
import dev.langchain4j.data.message.ChatMessage
import dev.langchain4j.model.StreamingResponseHandler
import dev.langchain4j.model.chat.StreamingChatLanguageModel
import dev.langchain4j.model.output.Response
import kotlinx.coroutines.channels.awaitClose
import kotlinx.coroutines.flow.Flow
import kotlinx.coroutines.flow.callbackFlow
import me.kpavlov.langchain4j.kotlin.internal.PII
import org.slf4j.LoggerFactory

private val logger = LoggerFactory.getLogger(StreamingChatLanguageModel::class.java)

/**
 * Represents different types of replies that can be received from an AI language model during streaming.
 * This sealed interface provides type-safe handling of both intermediate tokens and final completion responses.
 */
sealed interface StreamingChatLanguageModelReply {
    /**
     * Represents a single token received during the streaming process.
     *
     * @property token The individual token string generated by the model.
     */
    data class Token(
        val token: String,
    ) : StreamingChatLanguageModelReply

    /**
     * Represents the complete response received at the end of the streaming process.
     * This includes the final message along with any additional metadata.
     *
     * @property response The complete response containing the AI message and associated metadata.
     */
    data class Completion(
        val response: Response<AiMessage>,
    ) : StreamingChatLanguageModelReply
}

/**
 * Converts a streaming chat language model into a Kotlin [Flow] of [StreamingChatLanguageModelReply] events.
 * This extension function provides a coroutine-friendly way to consume streaming responses
 * from the language model.
 *
 * The flow emits individual tokens as they are generated, followed by a final completion event.
 * If an error occurs during generation, it will be propagated through the flow's error channel.
 *
 * Example usage:
 * ```kotlin
 * model.generateFlow(messages)
 *     .collect { reply ->
 *         when (reply) {
 *             is StreamingChatLanguageModel.Token -> println("Received token: ${reply.token}")
 *             is StreamingChatLanguageModel.Completion -> println("Generation complete: ${reply.response}")
 *         }
 *     }
 * ```
 *
 * @param messages The list of chat messages to send to the model. This typically includes
 *                the conversation history and the current prompt.
 * @return A [Flow] that emits [StreamingChatLanguageModelReply.Token] for each generated token and a final
 *         [StreamingChatLanguageModel.Completion] when generation is complete.
 * @throws Exception if an error occurs during generation. The error will be propagated
 *         through the flow's error channel.
 *
 * @see StreamingChatLanguageModelReply
 * @see StreamingChatLanguageModel
 * @see Flow
 */
fun StreamingChatLanguageModel.generateFlow(
    messages: List<ChatMessage>,
): Flow<StreamingChatLanguageModelReply> =
    callbackFlow {
        val model = this@generateFlow

        val handler =
            object : StreamingResponseHandler<AiMessage> {
                override fun onNext(token: String) {
                    logger.trace(
                        me.kpavlov.langchain4j.kotlin.internal.PII,
                        "Received token: {}",
                        token,
                    )
                    trySend(StreamingChatLanguageModelReply.Token(token))
                }

                override fun onComplete(response: Response<AiMessage>) {
                    logger.trace(
                        me.kpavlov.langchain4j.kotlin.internal.PII,
                        "Received response: {}",
                        response,
                    )
                    trySend(StreamingChatLanguageModelReply.Completion(response))
                    close()
                }

                override fun onError(error: Throwable) {
                    close(error)
                }
            }

        logger.info("Starting flow...")
        model.generate(messages, handler)

        // This will be called when the flow collection is closed or cancelled.
        awaitClose {
            // cleanup
            logger.info("Flow is canceled")
        }
    }
